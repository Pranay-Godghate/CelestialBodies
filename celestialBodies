# -*- coding: utf-8 -*-
"""MLDSA Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lVhDtO-ZHxupwGGPdJfTDe8VCvUhLpnh
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

"""Dataset

"""

data=pd.read_csv("/content/drive/MyDrive/SDSS.csv")
data.head(5)

"""EDA"""

data.describe()

data.columns

data.info()

data.shape

data.nunique()

data['class'].value_counts()

sns.countplot(x='class',data=data, palette='brg')
plt.show()

data.drop(['run','rerun'], axis=1, inplace=True)
data.head(3)

data.drop(['fiberid','specobjid','objid','mjd'],axis=1,inplace=True)
data.head(3)

data.drop(['camcol','field'],axis=1,inplace=True)
data.head(3)

corr=data.corr()
plt.figure(figsize=(12,10))
sns.heatmap(corr,annot=True)

sns.relplot(x='u',y='g',hue='class',data=data)

sns.relplot(x='ra',y='dec',hue='class',data=data)

sns.relplot(x='r',y='i',hue='class',data=data)

sns.relplot(x='u',y='i',hue='class',data=data)

sns.relplot(x='r',y='g',hue='class',data=data)

"""Preprocessing

"""

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
data['class']=le.fit_transform(data['class'])
class_mapping={ a:i for i,a in enumerate(le.classes_)}
print(class_mapping)

data

"""3.Splitting of dataset into training, validation and testing subsets.

"""

y=data['class']
X=data.drop('class',axis=1).values

from sklearn.model_selection import train_test_split
X1, X_test, y1, y_test=train_test_split(X,y,test_size=0.2,stratify=y,random_state=1)
X_train, X_val, y_train, y_val=train_test_split(X1,y1,test_size=0.25,random_state=2)

print("Train size is:",len(y_train))
print("validation size is:",len(y_val))
print("Test size is:",len(y_test))

feature_list=list(data.drop('class',axis=1).columns)

print(feature_list)

"""Feature selection

Varience Test
"""

from sklearn.feature_selection import VarianceThreshold
va=VarianceThreshold(threshold=0.1)
va.fit(X_train)
X_train_va=va.transform(X_train)
print(va.variances_)

"""Univariate

"""

from sklearn.feature_selection import mutual_info_classif
mic=mutual_info_classif(X_train,y_train,discrete_features=False,copy=True,n_neighbors=50,random_state=3)
print(mic)

sns.barplot(x=feature_list,y=mic)

from sklearn.feature_selection import SelectKBest
skb=SelectKBest(mutual_info_classif,k=8)
skb.fit(X_train,y_train)
X_train_skb=skb.transform(X_train)
for i,feat in enumerate(feature_list):
  print(feat+" :",skb.scores_[i])

"""Recursive feature elimination"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
rfc_feat_sel = RandomForestClassifier(n_estimators=200, max_depth=8, random_state=4)
rec_feat_elim = RFE(rfc_feat_sel, n_features_to_select=8, step=1)
rec_feat_elim.fit(X_train, y_train)
X_train_rfe = rec_feat_elim.transform(X_train)
ranks = np.argsort(rec_feat_elim.ranking_)
for r in ranks:
  print(feature_list[r] + " :", rec_feat_elim.ranking_[r])

"""Model Building

Decision Trees
"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(criterion='gini', max_depth=10, min_samples_leaf=10, class_weight={0:1.1, 1:1, 2:1.15}, random_state=5)
dt.fit(X_train, y_train)

from sklearn import tree
plt.figure(figsize=(5, 15), dpi=800)
tree.plot_tree(dt, filled=True, label=None, class_names=le.classes_)
plt.figure(figsize=(5, 15), dpi=800)
plt.show()

from sklearn.metrics import plot_confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
y_val_pred_dt = dt.predict(X_val)
as_dt = accuracy_score(y_val, y_val_pred_dt)
ps_dt = precision_score(y_val, y_val_pred_dt, average='weighted')
rs_dt = recall_score(y_val, y_val_pred_dt, average='weighted')
f1_dt = f1_score(y_val, y_val_pred_dt, average='weighted')
metric_dt = {"Accuracy Score": as_dt, "Precision Score": ps_dt, "Recall Score": rs_dt, "F1 Score": f1_dt}
print(metric_dt)

plot_confusion_matrix(dt, X_val, y_val)
plt.show()

"""Testing"""

from sklearn.metrics import plot_confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
y_test_pred_dt = dt.predict(X_test)
as_dt = accuracy_score(y_test, y_test_pred_dt)
ps_dt = precision_score(y_test, y_test_pred_dt, average='weighted')
rs_dt = recall_score(y_test, y_test_pred_dt, average='weighted')
f1_dt = f1_score(y_test, y_test_pred_dt, average='weighted')
metric_dt = {"Accuracy Score": as_dt, "Precision Score": ps_dt, "Recall Score": rs_dt, "F1 Score": f1_dt}
print(metric_dt)

"""Random Forest Classifier

"""

rf = RandomForestClassifier(n_estimators=700, max_depth=12, random_state=6)
rf.fit(X_train, y_train)

y_val_pred_rf = rf.predict(X_val)
as_rf = accuracy_score(y_val, y_val_pred_rf)
ps_rf = precision_score(y_val, y_val_pred_rf, average='weighted')
rs_rf = recall_score(y_val, y_val_pred_rf, average='weighted')
f1_rf = f1_score(y_val, y_val_pred_rf, average='weighted')
metric_rf = {"Accuracy Score": as_rf, "Precision Score": ps_rf, "Recall Score": rs_rf, "F1 Score": f1_rf}
print(metric_rf)

plot_confusion_matrix(rf, X_val, y_val)
plt.show()

"""Testing"""

y_test_pred_rf = rf.predict(X_test)
as_rf = accuracy_score(y_test, y_test_pred_rf)
ps_rf = precision_score(y_test, y_test_pred_rf, average='weighted')
rs_rf = recall_score(y_test, y_test_pred_rf, average='weighted')
f1_rf = f1_score(y_test, y_test_pred_rf, average='weighted')
metric_rf = {"Accuracy Score": as_rf, "Precision Score": ps_rf, "Recall Score": rs_rf, "F1 Score": f1_rf}
print(metric_rf)

plot_confusion_matrix(rf, X_test, y_test)
plt.show()
